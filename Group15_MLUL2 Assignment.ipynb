{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **PROJECT NAME**\n",
    "##### *PREDICTION ENGINE - RESTAURANTS*\n",
    "\n",
    "**GROUP DETAILS:** GROUP 15\n",
    "\n",
    "**TEAM MEMBERS**\n",
    "\n",
    "| Name                               \t| ID       \t|\n",
    "|:------------------------------------\t|:---------:|\n",
    "| Prashant Arya                        \t| 12010011 \t|\n",
    "| Abhilash Gadepalli                \t| 12010078 \t|\n",
    "| Debjit Ray                         \t| 12010066 \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:48:49.660479Z",
     "start_time": "2021-06-27T16:48:33.672016Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import random\n",
    "from functools import reduce\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import Reader, Dataset\n",
    "from surprise.prediction_algorithms.knns import KNNWithMeans\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.decomposition import NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:19.088675Z",
     "start_time": "2021-06-27T16:52:19.084212Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"RCData\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:19.158848Z",
     "start_time": "2021-06-27T16:52:19.091364Z"
    }
   },
   "outputs": [],
   "source": [
    "# import files\n",
    "chefmozaccepts = pd.read_csv(\"chefmozaccepts.csv\")\n",
    "chefmozcuisine = pd.read_csv(\"chefmozcuisine.csv\")\n",
    "chefmozhours = pd.read_csv(\"chefmozhours4.csv\")\n",
    "geoplaces = pd.read_csv(\"geoplaces2.csv\", encoding = \"latin-1\")\n",
    "\n",
    "rating = pd.read_csv(\"rating_final.csv\")\n",
    "\n",
    "usercuisine = pd.read_csv(\"usercuisine.csv\")\n",
    "userpayment = pd.read_csv(\"userpayment.csv\")\n",
    "user_profile = pd.read_csv(\"userprofile.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:22.086501Z",
     "start_time": "2021-06-27T16:52:22.036166Z"
    }
   },
   "outputs": [],
   "source": [
    "# get an overall rating by combining the 3 existing ratings\n",
    "rating[\"overall_rating\"] = rating.apply(lambda rating: rating[\"food_rating\"] + rating[\"service_rating\"] + rating[\"rating\"], \n",
    "                                        axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:26.918548Z",
     "start_time": "2021-06-27T16:52:26.872392Z"
    }
   },
   "outputs": [],
   "source": [
    "# get a user x restaurant (item) matrix\n",
    "# rating range is from 0 to 6; add offset of 10 and replace missing values with 0\n",
    "\n",
    "userPlaceMatrix = rating.pivot(index = \"userID\", columns = \"placeID\", values = \"overall_rating\")\n",
    "\n",
    "userPlaceMatrix = userPlaceMatrix.add(10)\n",
    "userPlaceMatrix = userPlaceMatrix.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:39.542986Z",
     "start_time": "2021-06-27T16:52:39.526228Z"
    }
   },
   "outputs": [],
   "source": [
    "def hybrid_model(rating):\n",
    "    \n",
    "    \"\"\"\n",
    "    Predicts ratings based on 4 methods: user-based collaborative filtering, item-based collaborative filtering, a weighted\n",
    "    average of both types of filtering, and a combination of either type based on the number of ratings for a user.\n",
    "    \n",
    "    Accepts a DataFrame with existing rating at a user-item level.\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data\n",
    "    reader = Reader(rating_scale = (0, 6))\n",
    "    data = Dataset.load_from_df(rating.filter([\"userID\", \"placeID\", \"overall_rating\"]), reader)\n",
    "    \n",
    "    # number of ratings for each user\n",
    "    user_n_ratings = rating.groupby([\"userID\"]).size().rename(\"n_ratings\").to_frame().sort_values(\"n_ratings\")\n",
    "    \n",
    "    # all possible user-item combinations\n",
    "    all_user_item_combo = pd.DataFrame(itertools.product(rating[\"userID\"].unique(), rating[\"placeID\"].unique()), \n",
    "                                   columns = [\"userID\", \"placeID\"])\n",
    "    \n",
    "    # unrated user item combinations\n",
    "    unrated_user_item_combo = all_user_item_combo.merge(rating, on = [\"userID\", \"placeID\"], how = \"left\")\n",
    "    unrated_user_item_combo = unrated_user_item_combo.loc[unrated_user_item_combo.isna().any(axis = 1), \n",
    "                                                          [\"userID\", \"placeID\"]]\n",
    "    # convert data to train set\n",
    "    trainset_full = data.build_full_trainset()\n",
    "    \n",
    "    # user-based collaborative filtering\n",
    "    sim_options = {\"name\": \"cosine\", \"user_based\": True}\n",
    "    knnb_u = KNNWithMeans(sim_options = sim_options)\n",
    "    _ = knnb_u.fit(trainset_full)\n",
    "    predictions_knnb_u = unrated_user_item_combo.apply(lambda row: knnb_u.predict(row[0], row[1]), axis = 1).tolist()\n",
    "\n",
    "    # item-based collaborative filtering\n",
    "    sim_options = {\"name\": \"cosine\", \"user_based\": False}\n",
    "    knnb_i = KNNWithMeans(sim_options = sim_options)\n",
    "    _ = knnb_i.fit(trainset_full)\n",
    "    predictions_knnb_i = unrated_user_item_combo.apply(lambda row: knnb_i.predict(row[0], row[1]), axis = 1).tolist()\n",
    "\n",
    "    # flag to indicate if recommendation is user based or item based\n",
    "    user_n_ratings[\"CF_type\"] = user_n_ratings[\"n_ratings\"].apply(lambda n: \"U\" if n > 2 else \"I\")\n",
    "\n",
    "    # merge datasets of both types of recommendations\n",
    "    user_item_hybrid = pd.merge(pd.DataFrame(predictions_knnb_i), pd.DataFrame(predictions_knnb_u), \n",
    "                                on = [\"uid\", \"iid\", \"r_ui\"], how = \"inner\", suffixes = (\"_item\", \"_user\"))\n",
    "    user_item_hybrid.drop([\"details_item\", \"details_user\"], axis = 1, inplace = True)\n",
    "\n",
    "    user_item_hybrid_flag = user_item_hybrid.merge(user_n_ratings, left_on = \"uid\", right_on = \"userID\")\n",
    "    \n",
    "    # get weighted prediction based on flag (assigned earlier)\n",
    "    user_item_hybrid_flag[\"weighted_prediction\"] = user_item_hybrid_flag.apply(lambda row: row[\"est_item\"]/4 + row[\"est_user\"]*0.75 \n",
    "                                                                               if row[\"CF_type\"] == \"U\" \n",
    "                                                                               else row[\"est_item\"]*0.75 + row[\"est_user\"]/4, \n",
    "                                                                               axis = 1)\n",
    "\n",
    "    # get either user-based prediction or item-based prediction based ont flag\n",
    "    user_item_hybrid_flag[\"non_weighted_prediction\"] = user_item_hybrid_flag.apply(lambda row: row[\"est_user\"] \n",
    "                                                                                   if row[\"CF_type\"] == \"U\" else row[\"est_item\"],\n",
    "                                                                                   axis = 1)\n",
    "    \n",
    "    user_item_hybrid_flag.rename(columns = {\"uid\": \"userID\", \"iid\": \"placeID\"}, inplace = True)\n",
    "    user_item_hybrid_flag.drop([\"r_ui\"], axis = 1, inplace = True)\n",
    "    \n",
    "    return user_item_hybrid_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:44.858002Z",
     "start_time": "2021-06-27T16:52:41.764929Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\mlul\\lib\\site-packages\\surprise\\prediction_algorithms\\algo_base.py:249: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  sim = construction_func[name](*args)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "hybrid_model_results = hybrid_model(rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine Prediction Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:49.754905Z",
     "start_time": "2021-06-27T16:52:49.742729Z"
    }
   },
   "outputs": [],
   "source": [
    "class DataScaler(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, feature_names):\n",
    "        self.sc = StandardScaler()\n",
    "        self.scaler = None\n",
    "        self.toscale_feature_names = feature_names\n",
    "\n",
    "    def fit(self, tmpDF, display = True):\n",
    "        if display:\n",
    "            print (\"StandardScaler: Fitting the scaler...\")\n",
    "        toscale_features = tmpDF[self.toscale_feature_names]\n",
    "        self.scaler = self.sc.fit(toscale_features.values)\n",
    "        return self\n",
    "\n",
    "    def transform(self, tmpDF, display = True):\n",
    "        if display:\n",
    "            print (\"StandardScaler: Scaling the data...\")\n",
    "        X = tmpDF.copy()\n",
    "        toscale_features = X[self.toscale_feature_names]\n",
    "        features = self.scaler.transform(toscale_features.values)\n",
    "        X[self.toscale_feature_names] = features\n",
    "        return pd.DataFrame(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T17:01:45.558885Z",
     "start_time": "2021-06-27T17:01:45.544906Z"
    }
   },
   "outputs": [],
   "source": [
    "def fncFindSim(tmpDF, fieldName):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the Cosine similarity scores for each pair of entities (userID or placeID)\n",
    "    \n",
    "    Accepts a dataframe of user x attributes matrix and then based on all the attributes calculates the similarities between\n",
    "    different users.\n",
    "    \"\"\"\n",
    "    \n",
    "    pairedDF = pd.DataFrame(columns = ['Entity_1', 'Entity_2', 'attr_1', 'attr_2', 'Cosine_Sim'])\n",
    "    \n",
    "    for currRowIdx in range(len(tmpDF)):\n",
    "        dict1 = {}\n",
    "        entity_1 = tmpDF.iloc[currRowIdx][fieldName]\n",
    "        attr_1 = tmpDF.iloc[currRowIdx,1:].values.tolist()\n",
    "        \n",
    "        for offset in range(len(tmpDF)):\n",
    "            entity_2 = tmpDF.iloc[offset][fieldName]\n",
    "            attr_2 = tmpDF.iloc[offset,1:].values.tolist()\n",
    "            Cosine_Sim = np.dot(attr_1, attr_2)/(np.linalg.norm(attr_1)*np.linalg.norm(attr_2))\n",
    "            dict1 = {'Entity_1': entity_1, 'Entity_2': entity_2, 'attr_1': attr_1, 'attr_2': attr_2, 'Cosine_Sim': Cosine_Sim}\n",
    "            pairedDF = pairedDF.append(dict1, ignore_index = True) \n",
    "            \n",
    "    return pairedDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:52:58.537459Z",
     "start_time": "2021-06-27T16:52:58.513272Z"
    }
   },
   "outputs": [],
   "source": [
    "def cosine_prediction(userPlaceMatrix, userProfile):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns the Cosine similarity based prediction for all places for each user\n",
    "    \n",
    "    Accepts a user x item matrix with imputed missing values    \n",
    "    \"\"\"\n",
    "\n",
    "    # Convert all categorical variables to dummy variables using one hot encoding.\n",
    "    fields =  ['smoker', 'drink_level', 'dress_preference', 'ambience', 'transport', 'marital_status', 'hijos', 'interest', 'personality', 'religion', 'activity', 'color', 'budget']\n",
    "    for fieldName in fields:\n",
    "        dummies = pd.get_dummies(userProfile[fieldName], prefix=fieldName, drop_first=True)\n",
    "        userProfile = pd.concat([userProfile, dummies], axis=1)\n",
    "        #Drop the original columns\n",
    "        userProfile.drop(fieldName, axis=1, inplace = True)\n",
    "\n",
    "    # Use the scaler to scale the original numeric fields\n",
    "    userScaler = DataScaler(feature_names = ['latitude', 'longitude','birth_year', 'weight', 'height'])\n",
    "    userScaler.fit(userProfile)\n",
    "    userProfile = userScaler.transform(userProfile)\n",
    "\n",
    "    # Calculate the Cosine similarity between each pair of UserID's\n",
    "    userPairDF = fncFindSim(userProfile, 'userID')\n",
    "    userSimilarityMatrix = userPairDF.pivot(index = \"Entity_1\", columns = \"Entity_2\", values = 'Cosine_Sim')\n",
    "\n",
    "    # Scale the user-place ratings matrix\n",
    "    scaler = StandardScaler()\n",
    "    scaleduserPlaceMatrix = pd.DataFrame(scaler.fit_transform(userPlaceMatrix), \n",
    "                                         index = userPlaceMatrix.index, columns = userPlaceMatrix.columns)\n",
    "\n",
    "    # Calculate the predictive rating based on Cosine similarity matrix and scaled user place rating matrix\n",
    "    userBasedrating = np.dot(userSimilarityMatrix, scaleduserPlaceMatrix)\n",
    "    userBasedrating = pd.DataFrame(userBasedrating,\n",
    "                                   index = userPlaceMatrix.index, columns = userPlaceMatrix.columns)\n",
    "\n",
    "    # Initiate a weight matrix with random weights\n",
    "    random.seed(10)\n",
    "    weights = pd.DataFrame(np.random.rand(userBasedrating.shape[1], userBasedrating.shape[1]))\n",
    "\n",
    "    # Parameters to tune\n",
    "    totErr = 0\n",
    "    alpha =  0.02\n",
    "    beta = 0.0002\n",
    "    max_iter = 5000\n",
    "    iter_cnt = 0\n",
    "\n",
    "    # Calculate the error matrix by subtracting the original scaled user-place ratings matrix and the calculated rating\n",
    "    # based on the cosine similarity between users and the past ratings available\n",
    "    errMatrix = np.subtract(np.dot(userBasedrating, weights.transpose()), scaleduserPlaceMatrix)\n",
    "    totErr = np.sum(np.sum(errMatrix))\n",
    "    prevErr = 999999\n",
    "\n",
    "    # Try to reduce the error by updating the weights\n",
    "    while iter_cnt < max_iter: \n",
    "        if ((totErr <= 500) and (totErr >= -500) or (prevErr == totErr)):\n",
    "            break\n",
    "        weights = beta + (alpha * weights)\n",
    "        errMatrix = np.subtract(np.dot(userBasedrating, weights.transpose()), scaleduserPlaceMatrix)\n",
    "        prevErr = totErr\n",
    "        totErr = np.sum(np.sum(errMatrix))\n",
    "        print (\"After {} iterations: Total Error: {}\".format(iter_cnt, totErr))\n",
    "        iter_cnt += 1\n",
    "\n",
    "    # Generate the predictive ratings\n",
    "    prediction = pd.DataFrame(np.dot(userBasedrating, weights), \n",
    "                              columns = userBasedrating.columns, index = userBasedrating.index)\n",
    "    \n",
    "    prediction_long = prediction.reset_index().melt(id_vars = [\"userID\"], var_name = \"placeID\", \n",
    "                                                   value_name = \"cosine_predictive_rating\")\n",
    "    \n",
    "    return prediction_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:12.043894Z",
     "start_time": "2021-06-27T16:53:01.763173Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardScaler: Fitting the scaler...\n",
      "StandardScaler: Scaling the data...\n",
      "After 0 iterations: Total Error: 2472.9129074874418\n",
      "After 1 iterations: Total Error: 97.15720465631276\n"
     ]
    }
   ],
   "source": [
    "cosine_prediction_results = cosine_prediction(userPlaceMatrix, user_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NMF Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:27.434151Z",
     "start_time": "2021-06-27T16:55:27.427008Z"
    }
   },
   "outputs": [],
   "source": [
    "def nmf_func(input_df):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns the NMF based prediction of all ratings\n",
    "    \n",
    "    Accepts a user x item matrix with imputed missing values    \n",
    "    \"\"\"\n",
    "\n",
    "    # get components\n",
    "    nmf_model = NMF(n_components=20)\n",
    "\n",
    "    # fit data and get W and H components\n",
    "    nmf_model.fit(input_df)\n",
    "    Theta = nmf_model.transform(input_df)\n",
    "    M = nmf_model.components_.T\n",
    "\n",
    "    # making the predictions\n",
    "    UserPlace_pred = Theta.dot(M.T)\n",
    "\n",
    "    UserPlace_pred = pd.DataFrame(UserPlace_pred, columns = input_df.columns, index = input_df.index).round(2)\n",
    "\n",
    "    UserPlace_pred_long = UserPlace_pred.reset_index().melt(id_vars = [\"userID\"], var_name = \"placeID\",\n",
    "                                                            value_name = \"nmf_predictive_rating\")\n",
    "\n",
    "    return UserPlace_pred_long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:30.385111Z",
     "start_time": "2021-06-27T16:55:30.086718Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP\\anaconda3\\envs\\mlul\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:312: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn((\"The 'init' value, when 'init=None' and \"\n"
     ]
    }
   ],
   "source": [
    "nmf_prediction_results = nmf_func(userPlaceMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:32.325555Z",
     "start_time": "2021-06-27T16:55:32.291197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userID</th>\n",
       "      <th>placeID</th>\n",
       "      <th>nmf_predictive_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>U1001</td>\n",
       "      <td>132560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U1002</td>\n",
       "      <td>132560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U1003</td>\n",
       "      <td>132560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>U1004</td>\n",
       "      <td>132560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>U1005</td>\n",
       "      <td>132560</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  userID placeID  nmf_predictive_rating\n",
       "0  U1001  132560                    0.0\n",
       "1  U1002  132560                    0.0\n",
       "2  U1003  132560                    0.0\n",
       "3  U1004  132560                    0.0\n",
       "4  U1005  132560                    0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nmf_prediction_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:37.250964Z",
     "start_time": "2021-06-27T16:55:37.182186Z"
    }
   },
   "outputs": [],
   "source": [
    "# merge outputs of all prediction methods\n",
    "pred_datasets = [hybrid_model_results, cosine_prediction_results, nmf_prediction_results]\n",
    "\n",
    "pred_merged_datasets = reduce(lambda x, y: pd.merge(x, y, on = [\"userID\", \"placeID\"]), pred_datasets).set_index([\"userID\", \"placeID\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:38.498089Z",
     "start_time": "2021-06-27T16:55:38.490798Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_rating_cols = [\"est_item\", \"est_user\", 'weighted_prediction', 'non_weighted_prediction', \n",
    "                    'cosine_predictive_rating', 'nmf_predictive_rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:55:40.933842Z",
     "start_time": "2021-06-27T16:55:40.913498Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_top_n(entity, by = \"userID\", n = 5):\n",
    "    \n",
    "    \"\"\"\n",
    "    Returns top n recommended items for a user or the top n recommended users for an item.\n",
    "    \n",
    "    entity: Either user or item ID\n",
    "    by: Type of entity i.e. userID or placeID\n",
    "    n: number of results\n",
    "    \"\"\"\n",
    "    \n",
    "    levels = [\"userID\", \"placeID\"]\n",
    "    \n",
    "    if by in levels:\n",
    "        \n",
    "        output = [col for col in levels if col != by][0]\n",
    "        \n",
    "        if (entity in pred_merged_datasets.index.get_level_values(by)):\n",
    "        \n",
    "            return pd.concat(map(lambda pred: pred_merged_datasets.xs(entity, \n",
    "                                                                      level = by)\\\n",
    "                                 .sort_values(by = pred, \n",
    "                                              ascending = False).head(n).reset_index()[output].rename(f\"by_{pred}\"), \n",
    "                                 pred_rating_cols), \n",
    "                             axis = 1)\n",
    "        else:\n",
    "            print(f\"{by[:-2].capitalize()} does not exist\")\n",
    "        \n",
    "    \n",
    "    else:\n",
    "        print(\"Incorrect Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-27T16:58:34.517319Z",
     "start_time": "2021-06-27T16:58:34.473033Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>by_est_item</th>\n",
       "      <th>by_est_user</th>\n",
       "      <th>by_weighted_prediction</th>\n",
       "      <th>by_non_weighted_prediction</th>\n",
       "      <th>by_cosine_predictive_rating</th>\n",
       "      <th>by_nmf_predictive_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>132958</td>\n",
       "      <td>132847</td>\n",
       "      <td>132755</td>\n",
       "      <td>132847</td>\n",
       "      <td>132875</td>\n",
       "      <td>135032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132955</td>\n",
       "      <td>135070</td>\n",
       "      <td>132847</td>\n",
       "      <td>135070</td>\n",
       "      <td>134999</td>\n",
       "      <td>135050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>132922</td>\n",
       "      <td>132846</td>\n",
       "      <td>135055</td>\n",
       "      <td>132846</td>\n",
       "      <td>132851</td>\n",
       "      <td>135041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>134986</td>\n",
       "      <td>132755</td>\n",
       "      <td>135034</td>\n",
       "      <td>132755</td>\n",
       "      <td>132937</td>\n",
       "      <td>135081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>135034</td>\n",
       "      <td>135055</td>\n",
       "      <td>132922</td>\n",
       "      <td>135055</td>\n",
       "      <td>132955</td>\n",
       "      <td>135062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>132755</td>\n",
       "      <td>135057</td>\n",
       "      <td>135057</td>\n",
       "      <td>135057</td>\n",
       "      <td>132921</td>\n",
       "      <td>135052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>135048</td>\n",
       "      <td>135034</td>\n",
       "      <td>132958</td>\n",
       "      <td>135034</td>\n",
       "      <td>132825</td>\n",
       "      <td>135063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>135080</td>\n",
       "      <td>132862</td>\n",
       "      <td>132955</td>\n",
       "      <td>132862</td>\n",
       "      <td>132862</td>\n",
       "      <td>135106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>135013</td>\n",
       "      <td>135054</td>\n",
       "      <td>135070</td>\n",
       "      <td>135054</td>\n",
       "      <td>132755</td>\n",
       "      <td>135079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>132954</td>\n",
       "      <td>135035</td>\n",
       "      <td>132846</td>\n",
       "      <td>135035</td>\n",
       "      <td>132922</td>\n",
       "      <td>135053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   by_est_item  by_est_user  by_weighted_prediction  \\\n",
       "0       132958       132847                  132755   \n",
       "1       132955       135070                  132847   \n",
       "2       132922       132846                  135055   \n",
       "3       134986       132755                  135034   \n",
       "4       135034       135055                  132922   \n",
       "5       132755       135057                  135057   \n",
       "6       135048       135034                  132958   \n",
       "7       135080       132862                  132955   \n",
       "8       135013       135054                  135070   \n",
       "9       132954       135035                  132846   \n",
       "\n",
       "   by_non_weighted_prediction  by_cosine_predictive_rating  \\\n",
       "0                      132847                       132875   \n",
       "1                      135070                       134999   \n",
       "2                      132846                       132851   \n",
       "3                      132755                       132937   \n",
       "4                      135055                       132955   \n",
       "5                      135057                       132921   \n",
       "6                      135034                       132825   \n",
       "7                      132862                       132862   \n",
       "8                      135054                       132755   \n",
       "9                      135035                       132922   \n",
       "\n",
       "   by_nmf_predictive_rating  \n",
       "0                    135032  \n",
       "1                    135050  \n",
       "2                    135041  \n",
       "3                    135081  \n",
       "4                    135062  \n",
       "5                    135052  \n",
       "6                    135063  \n",
       "7                    135106  \n",
       "8                    135079  \n",
       "9                    135053  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_top_n('U1099',by='userID',n =10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
